{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import h5py\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('..')\n",
    "import utils_HDF5 as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TriHiggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_triHiggs_h5_info(file_path):\n",
    "    # 印出 triHiggs HDF5 資料中，各 Higgs 數目的事件數\n",
    "    print(file_path)\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "\n",
    "        h1b1 = f['TARGETS/h1/b1'][...]\n",
    "        h1b2 = f['TARGETS/h1/b2'][...]\n",
    "        h2b1 = f['TARGETS/h2/b1'][...]\n",
    "        h2b2 = f['TARGETS/h2/b2'][...]\n",
    "        h3b1 = f['TARGETS/h3/b1'][...]\n",
    "        h3b2 = f['TARGETS/h3/b2'][...]\n",
    "\n",
    "        quark_jet = np.array([h1b1, h1b2, h2b1, h2b2, h3b1, h3b2]).T\n",
    "\n",
    "        h1_mask = utils.get_particle_mask(quark_jet, (0, 1))\n",
    "        h2_mask = utils.get_particle_mask(quark_jet, (2, 3))\n",
    "        h3_mask = utils.get_particle_mask(quark_jet, (4, 5))\n",
    "        \n",
    "        n_tot = h1_mask.shape[0]\n",
    "        n_0h = ((~h1_mask) & (~h2_mask) & (~h3_mask)).sum()\n",
    "        # 任一個 Higgs 有對應的 jet\n",
    "        n_1h = ((h1_mask & (~h2_mask) & (~h3_mask)) | \n",
    "                ((~h1_mask) & h2_mask & (~h3_mask)) | \n",
    "                ((~h1_mask) & (~h2_mask) & h3_mask)).sum()\n",
    "        \n",
    "        # 任兩個 Higgs 有對應的 jet\n",
    "        n_2h = ((h1_mask & h2_mask & (~h3_mask)) | \n",
    "                ((~h1_mask) & h2_mask & h3_mask) | \n",
    "                (h1_mask & (~h2_mask) & h3_mask)).sum()\n",
    "        n_3h = (h1_mask & h2_mask & h3_mask).sum()\n",
    "\n",
    "    print(f'Dataset size: {n_tot}')\n",
    "    print(f'Number of 0 Higgs events: {n_0h}')\n",
    "    print(f'Number of 1 Higgs events: {n_1h}')\n",
    "    print(f'Number of 2 Higgs events: {n_2h}')\n",
    "    print(f'Number of 3 Higgs events: {n_3h}')\n",
    "    \n",
    "    print(f'\\\\item Total sample size: {n_tot:,}')\n",
    "    print(f'\\\\item 1h sample size: {n_1h:,}')\n",
    "    print(f'\\\\item 2h sample size: {n_2h:,}')\n",
    "    print(f'\\\\item 3h sample size: {n_3h:,}')\n",
    "    \n",
    "    result = {\n",
    "        'total': n_tot,\n",
    "        '0h': n_0h,\n",
    "        '1h': n_1h,\n",
    "        '2h': n_2h,\n",
    "        '3h': n_3h\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def print_h5_sb_info(file):\n",
    "    # 印出訊號與背景的事件數\n",
    "    with h5py.File(file,'r') as f:\n",
    "        n_tot = f['CLASSIFICATIONS/EVENT/signal'][...].shape[0]\n",
    "        ns = (f['CLASSIFICATIONS/EVENT/signal'][...] == 1).sum()\n",
    "        nb = (f['CLASSIFICATIONS/EVENT/signal'][...] == 0).sum()\n",
    "\n",
    "    print(f'\\\\item Total sample size: {n_tot:,}')\n",
    "    print(f'\\\\item Signal sample size: {ns:,}')\n",
    "    print(f'\\\\item Background sample size: {nb:,}')\n",
    "    \n",
    "\n",
    "def select_3h_event(file, output_file):\n",
    "    # 選取 triHiggs HDF5 資料中，有 3 個 Higgs 的事件\n",
    "    # root, ext = os.path.splitext(file)\n",
    "    # new_file = root + '_3h' + ext\n",
    "\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        h1b1 = f['TARGETS/h1/b1'][...]\n",
    "        h1b2 = f['TARGETS/h1/b2'][...]\n",
    "        h2b1 = f['TARGETS/h2/b1'][...]\n",
    "        h2b2 = f['TARGETS/h2/b2'][...]\n",
    "        h3b1 = f['TARGETS/h3/b1'][...]\n",
    "        h3b2 = f['TARGETS/h3/b2'][...]\n",
    "\n",
    "        quark_jet = np.array([h1b1, h1b2, h2b1, h2b2, h3b1, h3b2]).T\n",
    "\n",
    "        h1_mask = utils.get_particle_mask(quark_jet, (0, 1))\n",
    "        h2_mask = utils.get_particle_mask(quark_jet, (2, 3))\n",
    "        h3_mask = utils.get_particle_mask(quark_jet, (4, 5))\n",
    "        \n",
    "        mask = h1_mask & h2_mask & h3_mask\n",
    "        n_3h = mask.sum()\n",
    "\n",
    "        print(f'Number of 3 Higgs events: {n_3h}')\n",
    "\n",
    "        # copy 3h events to new file\n",
    "        shutil.copyfile(file, output_file)\n",
    "        with h5py.File(output_file, 'a') as f_new:\n",
    "            for key in utils.get_dataset_keys(f):\n",
    "                f_new[key].resize(n_3h, axis=0)\n",
    "                f_new[key][:] = f[key][:][mask]\n",
    "\n",
    "    return output_file\n",
    "\n",
    "def select_nb_event(file, output_file, nb=6):\n",
    "    # 選取 triHiggs HDF5 資料中，有 nb 個 b-jets 的事件\n",
    "    # root, ext = os.path.splitext(file)\n",
    "\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        bTag = f['INPUTS/Source/btag'][...]\n",
    "        n_b_jet = np.sum(bTag, axis=1)\n",
    "\n",
    "        mask = n_b_jet >= nb\n",
    "        n_event = mask.sum()\n",
    "\n",
    "        print(f'Number of {nb} b events: {n_event}')\n",
    "\n",
    "        with h5py.File(output_file, 'w') as f_new:    \n",
    "            for key in utils.get_dataset_keys(f):\n",
    "                maxShape = list(f[key].maxshape)\n",
    "                maxShape[0] = None\n",
    "                f_new.create_dataset(key, maxshape=maxShape, data=f[key][:][mask])\n",
    "\n",
    "    return output_file\n",
    "\n",
    "def select_45b_event(file, output_file):\n",
    "    # 選取 triHiggs HDF5 資料中，有 4 或 5 個 b-jets 的事件\n",
    "    # root, ext = os.path.splitext(file)\n",
    "\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        bTag = f['INPUTS/Source/btag'][...]\n",
    "        n_b_jet = np.sum(bTag, axis=1)\n",
    "\n",
    "        mask = (n_b_jet == 4) | (n_b_jet == 5)\n",
    "        n_event = mask.sum()\n",
    "\n",
    "        print(f'Number of 4, 5 b events: {n_event}')\n",
    "\n",
    "        with h5py.File(output_file, 'w') as f_new:    \n",
    "            for key in utils.get_dataset_keys(f):\n",
    "                maxShape = list(f[key].maxshape)\n",
    "                maxShape[0] = None\n",
    "                f_new.create_dataset(key, maxshape=maxShape, data=f[key][:][mask])\n",
    "\n",
    "\n",
    "    return output_file\n",
    "\n",
    "def select_4pT40_event(file, output_file):\n",
    "    # 選取 triHiggs HDF5 資料中，有 nb 個 b-jets 的事件\n",
    "    # root, ext = os.path.splitext(file)\n",
    "\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        pt = f['INPUTS/Source/pt'][...]\n",
    "        pt_mask = pt[:, 3] > 40\n",
    "\n",
    "        n_event = pt_mask.sum()\n",
    "        print(f'Number of 4 pT > 40 GeV events: {n_event}')\n",
    "\n",
    "        # copy 3h events to new file\n",
    "        shutil.copyfile(file, output_file)\n",
    "        with h5py.File(output_file, 'a') as f_new:\n",
    "            for key in utils.get_dataset_keys(f):\n",
    "                f_new[key].resize(n_event, axis=0)\n",
    "                f_new[key][:] = f[key][:][pt_mask]\n",
    "\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./SPANet/bkg/pp6b-pT25_0b.h5\n",
      "Dataset size: 5570742\n",
      "CLASSIFICATIONS/EVENT/signal 0\n",
      "INPUTS/Source/MASK [ True  True  True  True  True  True False False False False False False\n",
      " False False False]\n",
      "INPUTS/Source/btag [False  True False False  True  True False False False False False False\n",
      " False False False]\n",
      "INPUTS/Source/eta [-1.5613574   1.7147591   0.50634855 -2.4052203  -2.3219755   1.6762717\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "INPUTS/Source/mass [18.679089   8.476983   9.943685   6.022513   7.6641026  3.0452356\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.       ]\n",
      "INPUTS/Source/phi [ 0.15385643  2.5918746   3.0130267  -1.8666257  -0.7792846   2.0502343\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "INPUTS/Source/pt [80.86402  63.279446 46.487667 33.09533  30.195988 25.177929  0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.      ]\n",
      "TARGETS/h1/b1 -1\n",
      "TARGETS/h1/b2 -1\n",
      "TARGETS/h2/b1 -1\n",
      "TARGETS/h2/b2 -1\n",
      "TARGETS/h3/b1 -1\n",
      "TARGETS/h3/b2 -1\n",
      "Number of 4 b events: 2431255\n",
      "./SPANet/bkg/pp6b-pT25_4b.h5\n",
      "Dataset size: 2431255\n",
      "Number of 0 Higgs events: 2431255\n",
      "Number of 1 Higgs events: 0\n",
      "Number of 2 Higgs events: 0\n",
      "Number of 3 Higgs events: 0\n",
      "\\item Total sample size: 2,431,255\n",
      "\\item 1h sample size: 0\n",
      "\\item 2h sample size: 0\n",
      "\\item 3h sample size: 0\n",
      "Number of 4 pT > 40 GeV events: 2030855\n",
      "./SPANet/bkg/pp6b-4pT40_4b.h5\n",
      "Dataset size: 2030855\n",
      "Number of 0 Higgs events: 2030855\n",
      "Number of 1 Higgs events: 0\n",
      "Number of 2 Higgs events: 0\n",
      "Number of 3 Higgs events: 0\n",
      "\\item Total sample size: 2,030,855\n",
      "\\item 1h sample size: 0\n",
      "\\item 2h sample size: 0\n",
      "\\item 3h sample size: 0\n",
      "Number of 6 b events: 139246\n",
      "./SPANet/bkg/pp6b-4pT40_6b.h5\n",
      "Dataset size: 139246\n",
      "Number of 0 Higgs events: 139246\n",
      "Number of 1 Higgs events: 0\n",
      "Number of 2 Higgs events: 0\n",
      "Number of 3 Higgs events: 0\n",
      "\\item Total sample size: 139,246\n",
      "\\item 1h sample size: 0\n",
      "\\item 2h sample size: 0\n",
      "\\item 3h sample size: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': 139246, '0h': 139246, '1h': 0, '2h': 0, '3h': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = './SPANet/bkg/pp6b-pT25_0b.h5'\n",
    "utils.print_h5_info(file_path, 600)\n",
    "\n",
    "output_file = './SPANet/bkg/pp6b-pT25_4b.h5'\n",
    "four_b_file = select_nb_event(file_path, output_file, nb=4)\n",
    "print_triHiggs_h5_info(four_b_file)\n",
    "\n",
    "output_file = './SPANet/bkg/pp6b-4pT40_4b.h5'\n",
    "four_pT40_file = select_4pT40_event(four_b_file, output_file)\n",
    "print_triHiggs_h5_info(four_pT40_file)\n",
    "\n",
    "output_file = './SPANet/bkg/pp6b-4pT40_6b.h5'\n",
    "six_b_file = select_nb_event(four_pT40_file, output_file, nb=6)\n",
    "print_triHiggs_h5_info(six_b_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/r10222035/SPANet2.3/data/triHiggs/triHiggs_TRSM-4pT40_4b-mix_5-train.h5\n",
      "Dataset size: 6000000\n",
      "Number of 0 Higgs events: 1234667\n",
      "Number of 1 Higgs events: 1200697\n",
      "Number of 2 Higgs events: 1985367\n",
      "Number of 3 Higgs events: 1579269\n",
      "\\item Total sample size: 6,000,000\n",
      "\\item 1h sample size: 1,200,697\n",
      "\\item 2h sample size: 1,985,367\n",
      "\\item 3h sample size: 1,579,269\n",
      "/home/r10222035/SPANet2.3/data/triHiggs/triHiggs_TRSM-4pT40_4b-mix_5-test.h5\n",
      "Dataset size: 300000\n",
      "Number of 0 Higgs events: 61689\n",
      "Number of 1 Higgs events: 59966\n",
      "Number of 2 Higgs events: 99105\n",
      "Number of 3 Higgs events: 79240\n",
      "\\item Total sample size: 300,000\n",
      "\\item 1h sample size: 59,966\n",
      "\\item 2h sample size: 99,105\n",
      "\\item 3h sample size: 79,240\n",
      "\\item Total sample size: 6,000,000\n",
      "\\item Signal sample size: 5,000,000\n",
      "\\item Background sample size: 1,000,000\n",
      "\\item Total sample size: 300,000\n",
      "\\item Signal sample size: 250,000\n",
      "\\item Background sample size: 50,000\n"
     ]
    }
   ],
   "source": [
    "spanet_data_dir = Path('~/SPANet2.3/data/triHiggs/').expanduser()\n",
    "print_triHiggs_h5_info(spanet_data_dir / 'triHiggs_TRSM-4pT40_4b-mix_5-train.h5')\n",
    "print_triHiggs_h5_info(spanet_data_dir / 'triHiggs_TRSM-4pT40_4b-mix_5-test.h5')\n",
    "\n",
    "print_h5_sb_info(spanet_data_dir / 'triHiggs_TRSM-4pT40_4b-mix_5-train.h5')\n",
    "print_h5_sb_info(spanet_data_dir / 'triHiggs_TRSM-4pT40_4b-mix_5-test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partcile_label(file_path):\n",
    "    print('Process particle mask.')\n",
    "    with h5py.File(file_path, 'r+') as f:\n",
    "        nevent = f['INPUTS/Source/pt'].shape[0]\n",
    "        h1b1 = f['TARGETS/h1/b1'][...]\n",
    "        h1b2 = f['TARGETS/h1/b2'][...]\n",
    "        h2b1 = f['TARGETS/h2/b1'][...]\n",
    "        h2b2 = f['TARGETS/h2/b2'][...]\n",
    "        h3b1 = f['TARGETS/h3/b1'][...]\n",
    "        h3b2 = f['TARGETS/h3/b2'][...]\n",
    "\n",
    "        quarks_Jet = np.array([h1b1, h1b2, h2b1, h2b2, h3b1, h3b2]).T\n",
    "        # 取得 quarks 的 mask\n",
    "        h1_mask = utils.get_particle_mask(quarks_Jet, (0, 1))\n",
    "        h2_mask = utils.get_particle_mask(quarks_Jet, (2, 3))\n",
    "        h3_mask = utils.get_particle_mask(quarks_Jet, (4, 5))\n",
    "\n",
    "        # 如果 quarks 的 mask 為 False，則將對應的 jet 設為 -1\n",
    "        f['TARGETS/h1/b1'][...] = np.where(h1_mask == False, -1, h1b1)\n",
    "        f['TARGETS/h1/b2'][...] = np.where(h1_mask == False, -1, h1b2)\n",
    "        f['TARGETS/h2/b1'][...] = np.where(h2_mask == False, -1, h2b1)\n",
    "        f['TARGETS/h2/b2'][...] = np.where(h2_mask == False, -1, h2b2)\n",
    "        f['TARGETS/h3/b1'][...] = np.where(h3_mask == False, -1, h3b1)\n",
    "        f['TARGETS/h3/b2'][...] = np.where(h3_mask == False, -1, h3b2)\n",
    "\n",
    "# replace all nan by 0\n",
    "def replace_nan_to_zero(file_path):\n",
    "    print('Replace all nan by 0.')\n",
    "    with h5py.File(file_path, 'r+') as f:\n",
    "        dataset_keys = utils.get_dataset_keys(f)\n",
    "        for key in dataset_keys:\n",
    "            data = f[key][...]\n",
    "            data[np.isnan(data)] = 0\n",
    "            f[key][...] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process particle mask.\n",
      "Replace all nan by 0.\n"
     ]
    }
   ],
   "source": [
    "file_path = './SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-99.h5'\n",
    "process_partcile_label(file_path)\n",
    "replace_nan_to_zero(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-99.h5\n",
      "Dataset size: 1214086\n",
      "CLASSIFICATIONS/EVENT/signal 1\n",
      "INPUTS/Source/MASK [ True  True  True  True  True  True False False False False False False\n",
      " False False False]\n",
      "INPUTS/Source/btag [False  True  True  True  True  True False False False False False False\n",
      " False False False]\n",
      "INPUTS/Source/eta [-2.3142445  -0.6806595  -1.0417455   0.91909975  0.34773642 -0.53816724\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "INPUTS/Source/mass [42.25673   12.320855  20.852262  15.874163   9.8029785  4.000685\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.       ]\n",
      "INPUTS/Source/phi [ 1.1038225 -1.2953217 -2.2855241  1.0666269 -2.6694667  2.4733875\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.       ]\n",
      "INPUTS/Source/pt [278.38937  210.61337  154.87965   89.28765   65.267235  36.080963\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.      ]\n",
      "TARGETS/h1/b1 -1\n",
      "TARGETS/h1/b2 -1\n",
      "TARGETS/h2/b1 4\n",
      "TARGETS/h2/b2 2\n",
      "TARGETS/h3/b1 -1\n",
      "TARGETS/h3/b2 -1\n"
     ]
    }
   ],
   "source": [
    "file_path = './SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-99.h5'\n",
    "utils.print_h5_info(file_path, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training and testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of SPANet/bkg/pp6b-4pT40_4b.h5: 2030855\n",
      "Size of SPANet/bkg/pp6b-4pT40_4b_split1.h5: 1050000\n",
      "Size of SPANet/bkg/pp6b-4pT40_4b_split2.h5: 980855\n",
      "Size of SPANet/bkg/pp6b-4pT40_4b-1.h5: 1050000\n",
      "Size of SPANet/bkg/pp6b-4pT40_4b-1_split1.h5: 1000000\n",
      "Size of SPANet/bkg/pp6b-4pT40_4b-1_split2.h5: 50000\n",
      "Number of 6 b events: 67436\n",
      "SPANet/bkg/pp6b-4pT40_6b.h5\n",
      "Dataset size: 67436\n",
      "Number of 0 Higgs events: 67436\n",
      "Number of 1 Higgs events: 0\n",
      "Number of 2 Higgs events: 0\n",
      "Number of 3 Higgs events: 0\n",
      "\\item Total sample size: 67,436\n",
      "\\item 1h sample size: 0\n",
      "\\item 2h sample size: 0\n",
      "\\item 3h sample size: 0\n",
      "Size of SPANet/bkg/pp6b-4pT40_6b.h5: 67436\n",
      "Size of SPANet/bkg/pp6b-4pT40_6b_split1.h5: 50000\n",
      "Size of SPANet/bkg/pp6b-4pT40_6b_split2.h5: 17436\n"
     ]
    }
   ],
   "source": [
    "bkg_dir = Path('./SPANet/bkg/')\n",
    "file_path = bkg_dir / 'pp6b-4pT40_4b.h5'\n",
    "\n",
    "# prepare 4b training and testing datasets\n",
    "train_size = 1000000\n",
    "test_size = 50000\n",
    "size = train_size + test_size\n",
    "utils.split_h5_size(file_path, size)\n",
    "\n",
    "root, ext = os.path.splitext(file_path)\n",
    "split_file1 = root + '_split1' + ext\n",
    "split_file2 = root + '_split2' + ext\n",
    "\n",
    "os.rename(split_file1, bkg_dir / 'pp6b-4pT40_4b-1.h5') # 4b train + test\n",
    "os.rename(split_file2, bkg_dir / 'pp6b-4pT40_4b-2.h5') # 6b test\n",
    "\n",
    "file_path = bkg_dir / 'pp6b-4pT40_4b-1.h5'\n",
    "utils.split_h5_size(file_path, train_size)\n",
    "\n",
    "root, ext = os.path.splitext(file_path)\n",
    "split_file1 = root + '_split1' + ext\n",
    "split_file2 = root + '_split2' + ext\n",
    "\n",
    "os.rename(split_file1, bkg_dir / 'pp6b-4pT40_4b-train.h5')\n",
    "os.rename(split_file2, bkg_dir / 'pp6b-4pT40_4b-test.h5')\n",
    "\n",
    "\n",
    "# prepare 6b testing sample\n",
    "output_file = bkg_dir / 'pp6b-4pT40_6b.h5'\n",
    "six_b_file = select_nb_event(bkg_dir / 'pp6b-4pT40_4b-2.h5', output_file, nb=6)\n",
    "print_triHiggs_h5_info(six_b_file)\n",
    "\n",
    "size = 50000\n",
    "utils.split_h5_size(six_b_file, size)\n",
    "\n",
    "root, ext = os.path.splitext(six_b_file)\n",
    "split_file1 = root + '_split1' + ext\n",
    "split_file2 = root + '_split2' + ext\n",
    "\n",
    "os.rename(split_file1, bkg_dir / 'pp6b-4pT40_6b-test.h5')\n",
    "\n",
    "# remove the split file\n",
    "os.remove(bkg_dir / 'pp6b-4pT40_4b-1.h5')\n",
    "os.remove(bkg_dir / 'pp6b-4pT40_4b-2.h5')\n",
    "os.remove(split_file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DM-CPV Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./Sample/SPANet/sig/gghhh_bsm_500_275/delphes_events_20241206_005920.h5' and ('./Sample/SPANet/sig/gghhh_bsm_500_275/delphes_events_20241206_191926.h5',) are same structure, can be merged.\n",
      "./Sample/SPANet/sig/gghhh_bsm_500_275/delphes_events_20241206_005920_merged.h5 not exist. Copy ./Sample/SPANet/sig/gghhh_bsm_500_275/delphes_events_20241206_005920.h5 to ./Sample/SPANet/sig/gghhh_bsm_500_275/delphes_events_20241206_005920_merged.h5\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_500_275/delphes_events_20241206_005920.h5: 263169\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_500_275/delphes_events_20241206_191926.h5: 397665\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_500_275/delphes_events_20241206_005920_merged.h5: 660834\n",
      "'./Sample/SPANet/sig/gghhh_bsm_500_300/delphes_events_20241209_222922.h5' and ('./Sample/SPANet/sig/gghhh_bsm_500_300/delphes_events_20241206_025512.h5',) are same structure, can be merged.\n",
      "./Sample/SPANet/sig/gghhh_bsm_500_300/delphes_events_20241209_222922_merged.h5 not exist. Copy ./Sample/SPANet/sig/gghhh_bsm_500_300/delphes_events_20241209_222922.h5 to ./Sample/SPANet/sig/gghhh_bsm_500_300/delphes_events_20241209_222922_merged.h5\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_500_300/delphes_events_20241209_222922.h5: 403190\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_500_300/delphes_events_20241206_025512.h5: 300697\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_500_300/delphes_events_20241209_222922_merged.h5: 703887\n",
      "'./Sample/SPANet/sig/gghhh_bsm_520_325/delphes_events_20241206_182813.h5' and ('./Sample/SPANet/sig/gghhh_bsm_520_325/delphes_events_20241209_221730.h5',) are same structure, can be merged.\n",
      "./Sample/SPANet/sig/gghhh_bsm_520_325/delphes_events_20241206_182813_merged.h5 not exist. Copy ./Sample/SPANet/sig/gghhh_bsm_520_325/delphes_events_20241206_182813.h5 to ./Sample/SPANet/sig/gghhh_bsm_520_325/delphes_events_20241206_182813_merged.h5\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_520_325/delphes_events_20241206_182813.h5: 317544\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_520_325/delphes_events_20241209_221730.h5: 420580\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_520_325/delphes_events_20241206_182813_merged.h5: 738124\n",
      "'./Sample/SPANet/sig/gghhh_bsm_570_250/delphes_events_20241228_015103.h5' and () are same structure, can be merged.\n",
      "./Sample/SPANet/sig/gghhh_bsm_570_250/delphes_events_20241228_015103_merged.h5 not exist. Copy ./Sample/SPANet/sig/gghhh_bsm_570_250/delphes_events_20241228_015103.h5 to ./Sample/SPANet/sig/gghhh_bsm_570_250/delphes_events_20241228_015103_merged.h5\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_570_250/delphes_events_20241228_015103.h5: 424764\n",
      "'./Sample/SPANet/sig/gghhh_bsm_600_325/delphes_events_20241220_041419.h5' and () are same structure, can be merged.\n",
      "./Sample/SPANet/sig/gghhh_bsm_600_325/delphes_events_20241220_041419_merged.h5 not exist. Copy ./Sample/SPANet/sig/gghhh_bsm_600_325/delphes_events_20241220_041419.h5 to ./Sample/SPANet/sig/gghhh_bsm_600_325/delphes_events_20241220_041419_merged.h5\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_600_325/delphes_events_20241220_041419.h5: 483595\n",
      "'./Sample/SPANet/sig/gghhh_bsm_700_325/delphes_events_20241220_013311.h5' and () are same structure, can be merged.\n",
      "./Sample/SPANet/sig/gghhh_bsm_700_325/delphes_events_20241220_013311_merged.h5 not exist. Copy ./Sample/SPANet/sig/gghhh_bsm_700_325/delphes_events_20241220_013311.h5 to ./Sample/SPANet/sig/gghhh_bsm_700_325/delphes_events_20241220_013311_merged.h5\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_700_325/delphes_events_20241220_013311.h5: 466599\n",
      "'./Sample/SPANet/sig/gghhh_bsm_800_325/delphes_events_20241223_180653.h5' and () are same structure, can be merged.\n",
      "./Sample/SPANet/sig/gghhh_bsm_800_325/delphes_events_20241223_180653_merged.h5 not exist. Copy ./Sample/SPANet/sig/gghhh_bsm_800_325/delphes_events_20241223_180653.h5 to ./Sample/SPANet/sig/gghhh_bsm_800_325/delphes_events_20241223_180653_merged.h5\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_800_325/delphes_events_20241223_180653.h5: 459197\n",
      "'./Sample/SPANet/sig/gghhh_bsm_700_400/delphes_events_20241223_220556.h5' and () are same structure, can be merged.\n",
      "./Sample/SPANet/sig/gghhh_bsm_700_400/delphes_events_20241223_220556_merged.h5 not exist. Copy ./Sample/SPANet/sig/gghhh_bsm_700_400/delphes_events_20241223_220556.h5 to ./Sample/SPANet/sig/gghhh_bsm_700_400/delphes_events_20241223_220556_merged.h5\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_700_400/delphes_events_20241223_220556.h5: 519502\n",
      "'./Sample/SPANet/sig/gghhh_bsm_800_400/delphes_events_20241223_185321.h5' and () are same structure, can be merged.\n",
      "./Sample/SPANet/sig/gghhh_bsm_800_400/delphes_events_20241223_185321_merged.h5 not exist. Copy ./Sample/SPANet/sig/gghhh_bsm_800_400/delphes_events_20241223_185321.h5 to ./Sample/SPANet/sig/gghhh_bsm_800_400/delphes_events_20241223_185321_merged.h5\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_800_400/delphes_events_20241223_185321.h5: 500610\n"
     ]
    }
   ],
   "source": [
    "# merge all .h5 files in the folder\n",
    "# for m3_m2 in ['500_275', '500_300', '520_325']:\n",
    "for m3_m2 in ['500_275', '500_300', '520_325', '570_250', '600_325', '700_325', '800_325', '700_400', '800_400']:\n",
    "    files_path = f'./Sample/SPANet/sig/gghhh_bsm_{m3_m2}/'\n",
    "    files = [os.path.join(files_path, name) for name in os.listdir(files_path) if name.startswith('delphes_events_')]\n",
    "    merged_h5 = utils.merge_h5_file(*files)\n",
    "\n",
    "    new_file = f'./Sample/SPANet/sig/gghhh_bsm_{m3_m2}/gghhh-4pT40_4b.h5'\n",
    "    os.rename(merged_h5, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m3_m2 in ['570_250', '600_325', '700_325', '800_325', '700_400', '800_400']:\n",
    "    file_name = f'./Sample/45b_400k/{m3_m2}_45b_400000.h5'\n",
    "    new_file = f'./Sample/SPANet/sig/gghhh_bsm_{m3_m2}/gghhh-4pT40_4b.h5'\n",
    "    # copy 3h events to new file\n",
    "    shutil.copyfile(file_name, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_4b.h5: 400000\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_4b_split1.h5: 83333\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_4b_split2.h5: 316667\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_4b-1.h5: 83333\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_4b-1_split1.h5: 74999\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_4b-1_split2.h5: 8334\n",
      "Number of 6 b events: 0\n",
      "Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_6b.h5\n",
      "Dataset size: 0\n",
      "Number of 0 Higgs events: 0\n",
      "Number of 1 Higgs events: 0\n",
      "Number of 2 Higgs events: 0\n",
      "Number of 3 Higgs events: 0\n",
      "\\item Total sample size: 0\n",
      "\\item 1h sample size: 0\n",
      "\\item 2h sample size: 0\n",
      "\\item 3h sample size: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_6b.h5: 0\n",
      "Split size 8333 is greater than the input file size 0.\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_6b_split1.h5: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_6b_split2.h5: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_4b.h5: 400000\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_4b_split1.h5: 83333\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_4b_split2.h5: 316667\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_4b-1.h5: 83333\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_4b-1_split1.h5: 74999\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_4b-1_split2.h5: 8334\n",
      "Number of 6 b events: 0\n",
      "Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_6b.h5\n",
      "Dataset size: 0\n",
      "Number of 0 Higgs events: 0\n",
      "Number of 1 Higgs events: 0\n",
      "Number of 2 Higgs events: 0\n",
      "Number of 3 Higgs events: 0\n",
      "\\item Total sample size: 0\n",
      "\\item 1h sample size: 0\n",
      "\\item 2h sample size: 0\n",
      "\\item 3h sample size: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_6b.h5: 0\n",
      "Split size 8333 is greater than the input file size 0.\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_6b_split1.h5: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_6b_split2.h5: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_4b.h5: 400000\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_4b_split1.h5: 83333\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_4b_split2.h5: 316667\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_4b-1.h5: 83333\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_4b-1_split1.h5: 74999\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_4b-1_split2.h5: 8334\n",
      "Number of 6 b events: 0\n",
      "Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_6b.h5\n",
      "Dataset size: 0\n",
      "Number of 0 Higgs events: 0\n",
      "Number of 1 Higgs events: 0\n",
      "Number of 2 Higgs events: 0\n",
      "Number of 3 Higgs events: 0\n",
      "\\item Total sample size: 0\n",
      "\\item 1h sample size: 0\n",
      "\\item 2h sample size: 0\n",
      "\\item 3h sample size: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_6b.h5: 0\n",
      "Split size 8333 is greater than the input file size 0.\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_6b_split1.h5: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_6b_split2.h5: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_4b.h5: 400000\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_4b_split1.h5: 83333\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_4b_split2.h5: 316667\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_4b-1.h5: 83333\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_4b-1_split1.h5: 74999\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_4b-1_split2.h5: 8334\n",
      "Number of 6 b events: 0\n",
      "Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_6b.h5\n",
      "Dataset size: 0\n",
      "Number of 0 Higgs events: 0\n",
      "Number of 1 Higgs events: 0\n",
      "Number of 2 Higgs events: 0\n",
      "Number of 3 Higgs events: 0\n",
      "\\item Total sample size: 0\n",
      "\\item 1h sample size: 0\n",
      "\\item 2h sample size: 0\n",
      "\\item 3h sample size: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_6b.h5: 0\n",
      "Split size 8333 is greater than the input file size 0.\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_6b_split1.h5: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_6b_split2.h5: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_4b.h5: 400000\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_4b_split1.h5: 83333\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_4b_split2.h5: 316667\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_4b-1.h5: 83333\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_4b-1_split1.h5: 74999\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_4b-1_split2.h5: 8334\n",
      "Number of 6 b events: 0\n",
      "Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_6b.h5\n",
      "Dataset size: 0\n",
      "Number of 0 Higgs events: 0\n",
      "Number of 1 Higgs events: 0\n",
      "Number of 2 Higgs events: 0\n",
      "Number of 3 Higgs events: 0\n",
      "\\item Total sample size: 0\n",
      "\\item 1h sample size: 0\n",
      "\\item 2h sample size: 0\n",
      "\\item 3h sample size: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_6b.h5: 0\n",
      "Split size 8333 is greater than the input file size 0.\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_6b_split1.h5: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_6b_split2.h5: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_4b.h5: 400000\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_4b_split1.h5: 83333\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_4b_split2.h5: 316667\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_4b-1.h5: 83333\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_4b-1_split1.h5: 74999\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_4b-1_split2.h5: 8334\n",
      "Number of 6 b events: 0\n",
      "Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_6b.h5\n",
      "Dataset size: 0\n",
      "Number of 0 Higgs events: 0\n",
      "Number of 1 Higgs events: 0\n",
      "Number of 2 Higgs events: 0\n",
      "Number of 3 Higgs events: 0\n",
      "\\item Total sample size: 0\n",
      "\\item 1h sample size: 0\n",
      "\\item 2h sample size: 0\n",
      "\\item 3h sample size: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_6b.h5: 0\n",
      "Split size 8333 is greater than the input file size 0.\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_6b_split1.h5: 0\n",
      "Size of Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_6b_split2.h5: 0\n"
     ]
    }
   ],
   "source": [
    "# m3_m2_list = ['420_280','500_275', '500_300', '520_325']\n",
    "m3_m2_list = ['420_280', '500_275', '500_300', '520_325',\n",
    "              '570_250', '600_325', '700_325',\n",
    "              '800_325', '700_400', '800_400'\n",
    "              ]\n",
    "n_mass = len(m3_m2_list)\n",
    "n_4b_total = 500000\n",
    "for m3_m2 in m3_m2_list:\n",
    "\n",
    "    # split signal files for 4b and 6b\n",
    "    file_dir = Path(f'./Sample/SPANet/sig/gghhh_bsm_{m3_m2}/')\n",
    "    file_path = file_dir / 'gghhh-4pT40_4b.h5'\n",
    "    size = n_4b_total // n_mass\n",
    "    utils.split_h5_size(file_path, size)\n",
    "\n",
    "    root, ext = os.path.splitext(file_path)\n",
    "    split_file1 = root + '_split1' + ext\n",
    "    split_file2 = root + '_split2' + ext\n",
    "\n",
    "    os.rename(split_file1, file_dir / 'gghhh-4pT40_4b-1.h5')\n",
    "    os.rename(split_file2, file_dir / 'gghhh-4pT40_4b-2.h5')\n",
    "\n",
    "    file_path = file_dir / 'gghhh-4pT40_4b-1.h5'\n",
    "    size = int(n_4b_total // n_mass * 0.9)\n",
    "    utils.split_h5_size(file_path, size)\n",
    "\n",
    "    root, ext = os.path.splitext(file_path)\n",
    "    split_file1 = root + '_split1' + ext\n",
    "    split_file2 = root + '_split2' + ext\n",
    "\n",
    "    os.rename(split_file1, file_dir / 'gghhh-4pT40_4b-train.h5')\n",
    "    os.rename(split_file2, file_dir / 'gghhh-4pT40_4b-test.h5')\n",
    "\n",
    "    output_file = file_dir / 'gghhh-4pT40_6b.h5'\n",
    "    six_b_file = select_nb_event(file_dir / 'gghhh-4pT40_4b-2.h5', output_file, nb=6)\n",
    "    print_triHiggs_h5_info(six_b_file)\n",
    "\n",
    "    size = 50000 // n_mass\n",
    "    utils.split_h5_size(six_b_file, size)\n",
    "\n",
    "    root, ext = os.path.splitext(six_b_file)\n",
    "    split_file1 = root + '_split1' + ext\n",
    "    split_file2 = root + '_split2' + ext\n",
    "\n",
    "    os.rename(split_file1, file_dir / 'gghhh-4pT40_6b-1.h5')\n",
    "    os.rename(split_file2, file_dir / 'gghhh-4pT40_6b-2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $4b$ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./Sample/SPANet/bkg/pp6b-4pT40_4b-train.h5' and ('./Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_4b-train.h5', './Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_4b-train.h5', './Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_4b-train.h5', './Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_4b-train.h5', './Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_4b-train.h5', './Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_4b-train.h5') are same structure, can be merged.\n",
      "./Sample/SPANet/bkg/pp6b-4pT40_4b-train_merged.h5 not exist. Copy ./Sample/SPANet/bkg/pp6b-4pT40_4b-train.h5 to ./Sample/SPANet/bkg/pp6b-4pT40_4b-train_merged.h5\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_4b-train.h5: 900000\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_4b-train.h5: 74999\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_4b-train_merged.h5: 974999\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_4b-train.h5: 74999\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_4b-train_merged.h5: 1049998\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_4b-train.h5: 74999\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_4b-train_merged.h5: 1124997\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_4b-train.h5: 74999\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_4b-train_merged.h5: 1199996\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_4b-train.h5: 74999\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_4b-train_merged.h5: 1274995\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_4b-train.h5: 74999\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_4b-train_merged.h5: 1349994\n",
      "'./Sample/SPANet/bkg/pp6b-4pT40_4b-test.h5' and ('./Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_4b-test.h5', './Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_4b-test.h5', './Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_4b-test.h5', './Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_4b-test.h5', './Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_4b-test.h5', './Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_4b-test.h5') are same structure, can be merged.\n",
      "./Sample/SPANet/bkg/pp6b-4pT40_4b-test_merged.h5 exist. Can not copy ./Sample/SPANet/bkg/pp6b-4pT40_4b-test.h5 to ./Sample/SPANet/bkg/pp6b-4pT40_4b-test_merged.h5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "rename: src should be string, bytes or os.PathLike, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n",
      "\u001b[1;32m     10\u001b[0m merged_h5 \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mmerge_h5_file(\u001b[38;5;241m*\u001b[39mfiles)\n",
      "\u001b[1;32m     12\u001b[0m new_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Sample/SPANet/triHiggs-4pT40_4b-mix_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_mass\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-test.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m---> 13\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_h5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: rename: src should be string, bytes or os.PathLike, not NoneType"
     ]
    }
   ],
   "source": [
    "files = ['./Sample/SPANet/bkg/pp6b-4pT40_4b-train.h5'] + [f'./Sample/SPANet/sig/gghhh_bsm_{m3_m2}/gghhh-4pT40_4b-train.h5' for m3_m2 in m3_m2_list]\n",
    "# files = [f'./Sample/SPANet/sig/gghhh_bsm_{m3_m2}/gghhh-4pT40_4b-train.h5' for m3_m2 in m3_m2_list]\n",
    "# files = [f'./Sample/45b_400k/{m3_m2}_45b_400000.h5' for m3_m2 in m3_m2_list]\n",
    "merged_h5 = utils.merge_h5_file(*files)\n",
    "\n",
    "new_file = f'./Sample/SPANet/triHiggs-4pT40_4b-mix_{n_mass}-train.h5'\n",
    "os.rename(merged_h5, new_file)\n",
    "\n",
    "files = ['./Sample/SPANet/bkg/pp6b-4pT40_4b-test.h5'] + [f'./Sample/SPANet/sig/gghhh_bsm_{m3_m2}/gghhh-4pT40_4b-test.h5' for m3_m2 in m3_m2_list]\n",
    "merged_h5 = utils.merge_h5_file(*files)\n",
    "\n",
    "new_file = f'./Sample/SPANet/triHiggs-4pT40_4b-mix_{n_mass}-test.h5'\n",
    "os.rename(merged_h5, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1349994\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m utils\u001b[38;5;241m.\u001b[39mshuffle_h5(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Sample/SPANet/triHiggs-4pT40_4b-mix_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_mass\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-train.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle_h5\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Sample/SPANet/triHiggs-4pT40_4b-mix_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn_mass\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-test.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/3h6b_collider_study/utils_HDF5.py:134\u001b[0m, in \u001b[0;36mshuffle_h5\u001b[0;34m(file_path)\u001b[0m\n",
      "\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;32m    133\u001b[0m     dataset_keys \u001b[38;5;241m=\u001b[39m get_dataset_keys(f)\n",
      "\u001b[0;32m--> 134\u001b[0m     nevent \u001b[38;5;241m=\u001b[39m f[\u001b[43mdataset_keys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnevent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;32m    137\u001b[0m     ind_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(nevent))\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "utils.shuffle_h5(f'./Sample/SPANet/triHiggs-4pT40_4b-mix_{n_mass}-train.h5')\n",
    "utils.shuffle_h5(f'./Sample/SPANet/triHiggs-4pT40_4b-mix_{n_mass}-test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $6b$ dataset: 50k + 50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./Sample/SPANet/bkg/pp6b-4pT40_6b-1.h5' and ('./Sample/SPANet/sig/gghhh_bsm_420_280/gghhh-4pT40_6b-1.h5', './Sample/SPANet/sig/gghhh_bsm_500_275/gghhh-4pT40_6b-1.h5', './Sample/SPANet/sig/gghhh_bsm_500_300/gghhh-4pT40_6b-1.h5', './Sample/SPANet/sig/gghhh_bsm_520_325/gghhh-4pT40_6b-1.h5', './Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_6b-1.h5', './Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_6b-1.h5', './Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_6b-1.h5', './Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_6b-1.h5', './Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_6b-1.h5', './Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_6b-1.h5') are same structure, can be merged.\n",
      "./Sample/SPANet/bkg/pp6b-4pT40_6b-1_merged.h5 not exist. Copy ./Sample/SPANet/bkg/pp6b-4pT40_6b-1.h5 to ./Sample/SPANet/bkg/pp6b-4pT40_6b-1_merged.h5\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_6b-1.h5: 50000\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_420_280/gghhh-4pT40_6b-1.h5: 5000\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_6b-1_merged.h5: 55000\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_500_275/gghhh-4pT40_6b-1.h5: 5000\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_6b-1_merged.h5: 60000\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_500_300/gghhh-4pT40_6b-1.h5: 5000\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_6b-1_merged.h5: 65000\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_520_325/gghhh-4pT40_6b-1.h5: 5000\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_6b-1_merged.h5: 70000\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_570_250/gghhh-4pT40_6b-1.h5: 5000\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_6b-1_merged.h5: 75000\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_600_325/gghhh-4pT40_6b-1.h5: 5000\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_6b-1_merged.h5: 80000\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_700_325/gghhh-4pT40_6b-1.h5: 5000\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_6b-1_merged.h5: 85000\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_800_325/gghhh-4pT40_6b-1.h5: 5000\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_6b-1_merged.h5: 90000\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_700_400/gghhh-4pT40_6b-1.h5: 5000\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_6b-1_merged.h5: 95000\n",
      "Size of ./Sample/SPANet/sig/gghhh_bsm_800_400/gghhh-4pT40_6b-1.h5: 5000\n",
      "Size of ./Sample/SPANet/bkg/pp6b-4pT40_6b-1_merged.h5: 100000\n"
     ]
    }
   ],
   "source": [
    "files = ['./Sample/SPANet/bkg/pp6b-4pT40_6b-1.h5'] + [f'./Sample/SPANet/sig/gghhh_bsm_{m3_m2}/gghhh-4pT40_6b-1.h5' for m3_m2 in m3_m2_list]\n",
    "merged_h5 = utils.merge_h5_file(*files)\n",
    "\n",
    "new_file = './Sample/SPANet/triHiggs-4pT40_6b-mix_10.h5'\n",
    "os.rename(merged_h5, new_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRSM signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-03.h5' and (PosixPath('SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-05.h5'), PosixPath('SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-01.h5'), PosixPath('SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-02.h5'), PosixPath('SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-04.h5'), PosixPath('SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-06.h5')) are same structure, can be merged.\n",
      "SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-03_merged.h5 not exist. Copy SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-03.h5 to SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-03_merged.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-03.h5: 324213\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-05.h5: 323620\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-03_merged.h5: 647833\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-01.h5: 3203\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-03_merged.h5: 651036\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-02.h5: 32385\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-03_merged.h5: 683421\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-04.h5: 323849\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-03_merged.h5: 1007270\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-06.h5: 323585\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-03_merged.h5: 1330855\n",
      "'SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-03.h5' and (PosixPath('SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-05.h5'), PosixPath('SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-01.h5'), PosixPath('SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-02.h5'), PosixPath('SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-04.h5')) are same structure, can be merged.\n",
      "SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-03_merged.h5 not exist. Copy SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-03.h5 to SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-03_merged.h5\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-03.h5: 392931\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-05.h5: 392255\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-03_merged.h5: 785186\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-01.h5: 3967\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-03_merged.h5: 789153\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-02.h5: 39132\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-03_merged.h5: 828285\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-04.h5: 392313\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-03_merged.h5: 1220598\n",
      "'SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-03.h5' and (PosixPath('SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-05.h5'), PosixPath('SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-01.h5'), PosixPath('SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-02.h5'), PosixPath('SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-04.h5')) are same structure, can be merged.\n",
      "SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-03_merged.h5 not exist. Copy SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-03.h5 to SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-03_merged.h5\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-03.h5: 398362\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-05.h5: 397974\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-03_merged.h5: 796336\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-01.h5: 4089\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-03_merged.h5: 800425\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-02.h5: 40081\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-03_merged.h5: 840506\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-04.h5: 397774\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-03_merged.h5: 1238280\n",
      "'SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-03.h5' and (PosixPath('SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-05.h5'), PosixPath('SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-01.h5'), PosixPath('SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-02.h5'), PosixPath('SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-04.h5')) are same structure, can be merged.\n",
      "SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-03_merged.h5 not exist. Copy SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-03.h5 to SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-03_merged.h5\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-03.h5: 416527\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-05.h5: 416375\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-03_merged.h5: 832902\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-01.h5: 4075\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-03_merged.h5: 836977\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-02.h5: 41857\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-03_merged.h5: 878834\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-04.h5: 416962\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-03_merged.h5: 1295796\n",
      "'SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-99.h5' and () are same structure, can be merged.\n",
      "SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-99_merged.h5 not exist. Copy SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-99.h5 to SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-99_merged.h5\n",
      "Size of SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-99.h5: 1214086\n"
     ]
    }
   ],
   "source": [
    "m3_m2_list = [(420, 280), (500, 275), (500, 300), (520, 325), (500, 350)]\n",
    "for m3, m2 in m3_m2_list:\n",
    "    files_path = Path(f'./SPANet/TRSM/TRSM_{m3}_{m2}/')\n",
    "\n",
    "    files = [files_path / name for name in os.listdir(files_path) if re.match(r'gghhh-4pT40_4b-\\d{2}.h5', name)]\n",
    "    merged_h5 = utils.merge_h5_file(*files)\n",
    "\n",
    "    new_file = files_path / 'gghhh-4pT40_4b.h5'\n",
    "    os.rename(merged_h5, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b.h5: 1330855\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b_split1.h5: 1050000\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b_split2.h5: 280855\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-1.h5: 1050000\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-1_split1.h5: 1000000\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-1_split2.h5: 50000\n",
      "Number of 6 b events: 34946\n",
      "SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b.h5\n",
      "Dataset size: 34946\n",
      "Number of 0 Higgs events: 463\n",
      "Number of 1 Higgs events: 3747\n",
      "Number of 2 Higgs events: 6963\n",
      "Number of 3 Higgs events: 23773\n",
      "\\item Total sample size: 34,946\n",
      "\\item 1h sample size: 3,747\n",
      "\\item 2h sample size: 6,963\n",
      "\\item 3h sample size: 23,773\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b.h5: 34946\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b_split1.h5: 10000\n",
      "Size of SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b_split2.h5: 24946\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b.h5: 1220598\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b_split1.h5: 1050000\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b_split2.h5: 170598\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-1.h5: 1050000\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-1_split1.h5: 1000000\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-1_split2.h5: 50000\n",
      "Number of 6 b events: 22316\n",
      "SPANet/TRSM/TRSM_500_275/gghhh-4pT40_6b.h5\n",
      "Dataset size: 22316\n",
      "Number of 0 Higgs events: 228\n",
      "Number of 1 Higgs events: 2262\n",
      "Number of 2 Higgs events: 4768\n",
      "Number of 3 Higgs events: 15058\n",
      "\\item Total sample size: 22,316\n",
      "\\item 1h sample size: 2,262\n",
      "\\item 2h sample size: 4,768\n",
      "\\item 3h sample size: 15,058\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_6b.h5: 22316\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_6b_split1.h5: 10000\n",
      "Size of SPANet/TRSM/TRSM_500_275/gghhh-4pT40_6b_split2.h5: 12316\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b.h5: 1238280\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b_split1.h5: 1050000\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b_split2.h5: 188280\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-1.h5: 1050000\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-1_split1.h5: 1000000\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-1_split2.h5: 50000\n",
      "Number of 6 b events: 24629\n",
      "SPANet/TRSM/TRSM_500_300/gghhh-4pT40_6b.h5\n",
      "Dataset size: 24629\n",
      "Number of 0 Higgs events: 259\n",
      "Number of 1 Higgs events: 2432\n",
      "Number of 2 Higgs events: 5494\n",
      "Number of 3 Higgs events: 16444\n",
      "\\item Total sample size: 24,629\n",
      "\\item 1h sample size: 2,432\n",
      "\\item 2h sample size: 5,494\n",
      "\\item 3h sample size: 16,444\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_6b.h5: 24629\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_6b_split1.h5: 10000\n",
      "Size of SPANet/TRSM/TRSM_500_300/gghhh-4pT40_6b_split2.h5: 14629\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b.h5: 1295796\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b_split1.h5: 1050000\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b_split2.h5: 245796\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-1.h5: 1050000\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-1_split1.h5: 1000000\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-1_split2.h5: 50000\n",
      "Number of 6 b events: 32511\n",
      "SPANet/TRSM/TRSM_520_325/gghhh-4pT40_6b.h5\n",
      "Dataset size: 32511\n",
      "Number of 0 Higgs events: 316\n",
      "Number of 1 Higgs events: 3096\n",
      "Number of 2 Higgs events: 7169\n",
      "Number of 3 Higgs events: 21930\n",
      "\\item Total sample size: 32,511\n",
      "\\item 1h sample size: 3,096\n",
      "\\item 2h sample size: 7,169\n",
      "\\item 3h sample size: 21,930\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_6b.h5: 32511\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_6b_split1.h5: 10000\n",
      "Size of SPANet/TRSM/TRSM_520_325/gghhh-4pT40_6b_split2.h5: 22511\n",
      "Size of SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b.h5: 1214086\n",
      "Size of SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b_split1.h5: 1050000\n",
      "Size of SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b_split2.h5: 164086\n",
      "Size of SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-1.h5: 1050000\n",
      "Size of SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-1_split1.h5: 1000000\n",
      "Size of SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-1_split2.h5: 50000\n",
      "Number of 6 b events: 21963\n",
      "SPANet/TRSM/TRSM_500_350/gghhh-4pT40_6b.h5\n",
      "Dataset size: 21963\n",
      "Number of 0 Higgs events: 45\n",
      "Number of 1 Higgs events: 904\n",
      "Number of 2 Higgs events: 6478\n",
      "Number of 3 Higgs events: 14536\n",
      "\\item Total sample size: 21,963\n",
      "\\item 1h sample size: 904\n",
      "\\item 2h sample size: 6,478\n",
      "\\item 3h sample size: 14,536\n",
      "Size of SPANet/TRSM/TRSM_500_350/gghhh-4pT40_6b.h5: 21963\n",
      "Size of SPANet/TRSM/TRSM_500_350/gghhh-4pT40_6b_split1.h5: 10000\n",
      "Size of SPANet/TRSM/TRSM_500_350/gghhh-4pT40_6b_split2.h5: 11963\n"
     ]
    }
   ],
   "source": [
    "n_mass = len(m3_m2_list)\n",
    "size_4b_train = 1000000\n",
    "size_4b_test = 50000\n",
    "size_4b = size_4b_train + size_4b_test\n",
    "size_6b = 10000\n",
    "for m3, m2 in m3_m2_list:\n",
    "\n",
    "    # split signal files for 4b and 6b\n",
    "    file_dir = Path(f'./SPANet/TRSM/TRSM_{m3}_{m2}/')\n",
    "    file_path = file_dir / 'gghhh-4pT40_4b.h5'\n",
    "    utils.split_h5_size(file_path, size_4b)\n",
    "\n",
    "    root, ext = os.path.splitext(file_path)\n",
    "    split_file1 = root + '_split1' + ext\n",
    "    split_file2 = root + '_split2' + ext\n",
    "\n",
    "    os.rename(split_file1, file_dir / 'gghhh-4pT40_4b-1.h5')\n",
    "    os.rename(split_file2, file_dir / 'gghhh-4pT40_4b-2.h5')\n",
    "\n",
    "    file_path = file_dir / 'gghhh-4pT40_4b-1.h5'\n",
    "    utils.split_h5_size(file_path, size_4b_train)\n",
    "\n",
    "    root, ext = os.path.splitext(file_path)\n",
    "    split_file1 = root + '_split1' + ext\n",
    "    split_file2 = root + '_split2' + ext\n",
    "\n",
    "    os.rename(split_file1, file_dir / 'gghhh-4pT40_4b-train.h5')\n",
    "    os.rename(split_file2, file_dir / 'gghhh-4pT40_4b-test.h5')\n",
    "\n",
    "    output_file = file_dir / 'gghhh-4pT40_6b.h5'\n",
    "    six_b_file = select_nb_event(file_dir / 'gghhh-4pT40_4b-2.h5', output_file, nb=6)\n",
    "    print_triHiggs_h5_info(six_b_file)\n",
    "\n",
    "    utils.split_h5_size(six_b_file, size_6b)\n",
    "\n",
    "    root, ext = os.path.splitext(six_b_file)\n",
    "    split_file1 = root + '_split1' + ext\n",
    "    split_file2 = root + '_split2' + ext\n",
    "\n",
    "    os.rename(split_file1, file_dir / 'gghhh-4pT40_6b-test.h5')\n",
    "\n",
    "    # remove the split file\n",
    "    os.remove(file_dir / 'gghhh-4pT40_4b-1.h5')\n",
    "    os.remove(file_dir / 'gghhh-4pT40_4b-2.h5')\n",
    "    os.remove(split_file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $4b$ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./SPANet/bkg/pp6b-4pT40_4b-train.h5' and ('./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-train.h5', './SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-train.h5', './SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-train.h5', './SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-train.h5', './SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-train.h5') are same structure, can be merged.\n",
      "./SPANet/bkg/pp6b-4pT40_4b-train_merged.h5 not exist. Copy ./SPANet/bkg/pp6b-4pT40_4b-train.h5 to ./SPANet/bkg/pp6b-4pT40_4b-train_merged.h5\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_4b-train.h5: 1000000\n",
      "Size of ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-train.h5: 1000000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_4b-train_merged.h5: 2000000\n",
      "Size of ./SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-train.h5: 1000000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_4b-train_merged.h5: 3000000\n",
      "Size of ./SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-train.h5: 1000000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_4b-train_merged.h5: 4000000\n",
      "Size of ./SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-train.h5: 1000000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_4b-train_merged.h5: 5000000\n",
      "Size of ./SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-train.h5: 1000000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_4b-train_merged.h5: 6000000\n",
      "Dataset size: 6000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/r10222035/SPANet2.3/data/triHiggs/triHiggs_TRSM-4pT40_4b-mix_5-train.h5')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = ['./SPANet/bkg/pp6b-4pT40_4b-train.h5'] + [f'./SPANet/TRSM/TRSM_{m3}_{m2}/gghhh-4pT40_4b-train.h5' for m3, m2 in m3_m2_list]\n",
    "merged_h5 = utils.merge_h5_file(*files)\n",
    "\n",
    "train_file = f'./SPANet/TRSM/triHiggs_TRSM-4pT40_4b-mix_{n_mass}-train.h5'\n",
    "os.rename(merged_h5, train_file)\n",
    "\n",
    "utils.shuffle_h5(train_file)\n",
    "\n",
    "spanet_data_dir = Path('~/SPANet2.3/data/triHiggs/').expanduser()\n",
    "shutil.copyfile(train_file, spanet_data_dir / 'triHiggs_TRSM-4pT40_4b-mix_5-train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-test.h5' and ('./SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-test.h5', './SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-test.h5', './SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-test.h5', './SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-test.h5') are same structure, can be merged.\n",
      "./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-test_merged.h5 not exist. Copy ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-test.h5 to ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-test_merged.h5\n",
      "Size of ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-test.h5: 50000\n",
      "Size of ./SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-test.h5: 50000\n",
      "Size of ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-test_merged.h5: 100000\n",
      "Size of ./SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-test.h5: 50000\n",
      "Size of ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-test_merged.h5: 150000\n",
      "Size of ./SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-test.h5: 50000\n",
      "Size of ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-test_merged.h5: 200000\n",
      "Size of ./SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-test.h5: 50000\n",
      "Size of ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-test_merged.h5: 250000\n",
      "'./SPANet/bkg/pp6b-4pT40_4b-test.h5' and ('./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-test.h5', './SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-test.h5', './SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-test.h5', './SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-test.h5', './SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-test.h5') are same structure, can be merged.\n",
      "./SPANet/bkg/pp6b-4pT40_4b-test_merged.h5 not exist. Copy ./SPANet/bkg/pp6b-4pT40_4b-test.h5 to ./SPANet/bkg/pp6b-4pT40_4b-test_merged.h5\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_4b-test.h5: 50000\n",
      "Size of ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_4b-test.h5: 50000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_4b-test_merged.h5: 100000\n",
      "Size of ./SPANet/TRSM/TRSM_500_275/gghhh-4pT40_4b-test.h5: 50000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_4b-test_merged.h5: 150000\n",
      "Size of ./SPANet/TRSM/TRSM_500_300/gghhh-4pT40_4b-test.h5: 50000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_4b-test_merged.h5: 200000\n",
      "Size of ./SPANet/TRSM/TRSM_520_325/gghhh-4pT40_4b-test.h5: 50000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_4b-test_merged.h5: 250000\n",
      "Size of ./SPANet/TRSM/TRSM_500_350/gghhh-4pT40_4b-test.h5: 50000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_4b-test_merged.h5: 300000\n"
     ]
    }
   ],
   "source": [
    "files = [f'./SPANet/TRSM/TRSM_{m3}_{m2}/gghhh-4pT40_4b-test.h5' for m3, m2 in m3_m2_list]\n",
    "merged_h5 = utils.merge_h5_file(*files)\n",
    "\n",
    "signal_file = f'./SPANet/TRSM/gghhh-4pT40_4b-mix_{n_mass}-test.h5'\n",
    "os.rename(merged_h5, signal_file)\n",
    "\n",
    "files = ['./SPANet/bkg/pp6b-4pT40_4b-test.h5'] + [f'./SPANet/TRSM/TRSM_{m3}_{m2}/gghhh-4pT40_4b-test.h5' for m3, m2 in m3_m2_list]\n",
    "merged_h5 = utils.merge_h5_file(*files)\n",
    "\n",
    "test_file = f'./SPANet/TRSM/triHiggs_TRSM-4pT40_4b-mix_{n_mass}-test.h5'\n",
    "os.rename(merged_h5, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $6b$ dataset: 50k + 50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b-test.h5' and ('./SPANet/TRSM/TRSM_500_275/gghhh-4pT40_6b-test.h5', './SPANet/TRSM/TRSM_500_300/gghhh-4pT40_6b-test.h5', './SPANet/TRSM/TRSM_520_325/gghhh-4pT40_6b-test.h5', './SPANet/TRSM/TRSM_500_350/gghhh-4pT40_6b-test.h5') are same structure, can be merged.\n",
      "./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b-test_merged.h5 not exist. Copy ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b-test.h5 to ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b-test_merged.h5\n",
      "Size of ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b-test.h5: 10000\n",
      "Size of ./SPANet/TRSM/TRSM_500_275/gghhh-4pT40_6b-test.h5: 10000\n",
      "Size of ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b-test_merged.h5: 20000\n",
      "Size of ./SPANet/TRSM/TRSM_500_300/gghhh-4pT40_6b-test.h5: 10000\n",
      "Size of ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b-test_merged.h5: 30000\n",
      "Size of ./SPANet/TRSM/TRSM_520_325/gghhh-4pT40_6b-test.h5: 10000\n",
      "Size of ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b-test_merged.h5: 40000\n",
      "Size of ./SPANet/TRSM/TRSM_500_350/gghhh-4pT40_6b-test.h5: 10000\n",
      "Size of ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b-test_merged.h5: 50000\n",
      "'./SPANet/bkg/pp6b-4pT40_6b-test.h5' and ('./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b-test.h5', './SPANet/TRSM/TRSM_500_275/gghhh-4pT40_6b-test.h5', './SPANet/TRSM/TRSM_500_300/gghhh-4pT40_6b-test.h5', './SPANet/TRSM/TRSM_520_325/gghhh-4pT40_6b-test.h5', './SPANet/TRSM/TRSM_500_350/gghhh-4pT40_6b-test.h5') are same structure, can be merged.\n",
      "./SPANet/bkg/pp6b-4pT40_6b-test_merged.h5 not exist. Copy ./SPANet/bkg/pp6b-4pT40_6b-test.h5 to ./SPANet/bkg/pp6b-4pT40_6b-test_merged.h5\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_6b-test.h5: 50000\n",
      "Size of ./SPANet/TRSM/TRSM_420_280/gghhh-4pT40_6b-test.h5: 10000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_6b-test_merged.h5: 60000\n",
      "Size of ./SPANet/TRSM/TRSM_500_275/gghhh-4pT40_6b-test.h5: 10000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_6b-test_merged.h5: 70000\n",
      "Size of ./SPANet/TRSM/TRSM_500_300/gghhh-4pT40_6b-test.h5: 10000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_6b-test_merged.h5: 80000\n",
      "Size of ./SPANet/TRSM/TRSM_520_325/gghhh-4pT40_6b-test.h5: 10000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_6b-test_merged.h5: 90000\n",
      "Size of ./SPANet/TRSM/TRSM_500_350/gghhh-4pT40_6b-test.h5: 10000\n",
      "Size of ./SPANet/bkg/pp6b-4pT40_6b-test_merged.h5: 100000\n"
     ]
    }
   ],
   "source": [
    "files = [f'./SPANet/TRSM/TRSM_{m3}_{m2}/gghhh-4pT40_6b-test.h5' for m3, m2 in m3_m2_list]\n",
    "merged_h5 = utils.merge_h5_file(*files)\n",
    "\n",
    "signal_file = f'./SPANet/TRSM/gghhh-4pT40_6b-mix_{n_mass}-test.h5'\n",
    "os.rename(merged_h5, signal_file)\n",
    "\n",
    "files = ['./SPANet/bkg/pp6b-4pT40_6b-test.h5'] + [f'./SPANet/TRSM/TRSM_{m3}_{m2}/gghhh-4pT40_6b-test.h5' for m3, m2 in m3_m2_list]\n",
    "merged_h5 = utils.merge_h5_file(*files)\n",
    "\n",
    "new_file = f'./SPANet/TRSM/triHiggs_TRSM-4pT40_6b-mix_{n_mass}-test.h5'\n",
    "os.rename(merged_h5, new_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process SPANet predict file for pairing and DNN\n",
    "Change the dataset name and replace the predict results by true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_labels(file_path, label_path):\n",
    "    with h5py.File(label_path, 'r') as f:\n",
    "        label = f['CLASSIFICATIONS/EVENT/signal'][...]\n",
    "\n",
    "    with h5py.File(file_path, 'r+') as f:\n",
    "        if 'CLASSIFICATIONS/EVENT/signal' in f:\n",
    "            del f['CLASSIFICATIONS/EVENT/signal']\n",
    "        f.create_dataset('CLASSIFICATIONS/EVENT/signal', data=label, chunks=True, maxshape=(None,))\n",
    "\n",
    "def rename_dataset(file_path):\n",
    "    with h5py.File(file_path, 'r+') as f:\n",
    "        for key in utils.get_dataset_keys(f):\n",
    "            if key.startswith('SpecialKey.'):\n",
    "                new_key = key.replace('SpecialKey.', '')\n",
    "            else:\n",
    "                new_key = key\n",
    "            # first word capitalize\n",
    "            new_key = new_key.split('/')\n",
    "            new_key = '/'.join([new_key[0].upper()] + new_key[1:])\n",
    "            if new_key == key:\n",
    "                continue\n",
    "            \n",
    "            maxShape = list(f[key].maxshape)\n",
    "            maxShape[0] = None\n",
    "            f.create_dataset(new_key, data=f[key][...], chunks=True, maxshape=maxShape)\n",
    "            del f[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DM-CPV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = './Sample/SPANet/TRSM/'\n",
    "rename_dataset('Sample/SPANet/triHiggs-4pT40_4b-mix-train-4b_SPANet_pairing.h5')\n",
    "rename_dataset('Sample/SPANet/triHiggs-4pT40_4b-mix-test-4b_SPANet_pairing.h5')\n",
    "\n",
    "file_path = 'Sample/SPANet/triHiggs-4pT40_4b-mix-train-4b_SPANet_pairing.h5'\n",
    "label_path = 'Sample/SPANet/triHiggs-4pT40_4b-mix-train.h5'\n",
    "replace_labels(file_path, label_path)\n",
    "\n",
    "file_path = 'Sample/SPANet/triHiggs-4pT40_4b-mix-test-4b_SPANet_pairing.h5'\n",
    "label_path = 'Sample/SPANet/triHiggs-4pT40_4b-mix-test.h5'\n",
    "replace_labels(file_path, label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_dir = sample_dir / 'sig'\n",
    "for m3_m2 in ['420_280', '500_275', '500_300', '520_325']:\n",
    "    file_path = sig_dir / f'gghhh_bsm_{m3_m2}/gghhh-4pT40_4b-test-4b_SPANet_pairing.h5'\n",
    "    label_path = sig_dir / f'gghhh_bsm_{m3_m2}/gghhh-4pT40_4b-test.h5'\n",
    "    rename_dataset(file_path)\n",
    "    replace_labels(file_path, label_path)\n",
    "\n",
    "    file_path = sig_dir / f'gghhh_bsm_{m3_m2}/gghhh-4pT40_6b-1-4b_SPANet_pairing.h5'\n",
    "    label_path = sig_dir / f'gghhh_bsm_{m3_m2}/gghhh-4pT40_6b-1.h5'\n",
    "    rename_dataset(file_path)\n",
    "    replace_labels(file_path, label_path)\n",
    "\n",
    "file_path = './Sample/SPANet/bkg/pp6b-4pT40_4b-test-4b_SPANet_pairing.h5'\n",
    "label_path = './Sample/SPANet/bkg/pp6b-4pT40_4b-test.h5'\n",
    "rename_dataset(file_path)\n",
    "replace_labels(file_path, label_path)\n",
    "\n",
    "file_path = './Sample/SPANet/bkg/pp6b-4pT40_6b-1-4b_SPANet_pairing.h5'\n",
    "label_path = './Sample/SPANet/bkg/pp6b-4pT40_6b-1.h5'\n",
    "rename_dataset(file_path)\n",
    "replace_labels(file_path, label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Sample/SPANet/triHiggs-4pT40_6b-mix-4b_SPANet_pairing.h5'\n",
    "label_path = 'Sample/SPANet/triHiggs-4pT40_6b-mix.h5'\n",
    "rename_dataset(file_path)\n",
    "replace_labels(file_path, label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for DNN training\n",
    "sample_dir = Path('./Sample/SPANet/TRSM/')\n",
    "\n",
    "file_path = sample_dir / 'triHiggs_TRSM-4pT40_4b-mix_5-train-4b_SPANet_pairing.h5'\n",
    "label_path = sample_dir / 'triHiggs_TRSM-4pT40_4b-mix_5-train.h5'\n",
    "rename_dataset(file_path)\n",
    "replace_labels(file_path, label_path)\n",
    "\n",
    "file_path = sample_dir / 'triHiggs_TRSM-4pT40_4b-mix_5-test-4b_SPANet_pairing.h5'\n",
    "label_path = sample_dir / 'triHiggs_TRSM-4pT40_4b-mix_5-test.h5'\n",
    "rename_dataset(file_path)\n",
    "replace_labels(file_path, label_path)\n",
    "\n",
    "file_path = sample_dir / 'triHiggs_TRSM-4pT40_6b-mix_5-4b_SPANet_pairing.h5'\n",
    "label_path = sample_dir / 'triHiggs_TRSM-4pT40_6b-mix_5.h5'\n",
    "rename_dataset(file_path)\n",
    "replace_labels(file_path, label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluating AUC on each mass point\n",
    "for m3, m2 in [(420, 280), (500, 275), (500, 300), (520, 325), (500, 350)]:\n",
    "    sig_dir = sample_dir / f'TRSM_{m3}_{m2}'\n",
    "    file_path = sig_dir / f'gghhh-4pT40_4b-test-mix_5-4b_SPANet_pairing.h5'\n",
    "    label_path = sig_dir / f'gghhh-4pT40_4b-test.h5'\n",
    "    rename_dataset(file_path)\n",
    "    replace_labels(file_path, label_path)\n",
    "\n",
    "    file_path = sig_dir / f'gghhh-4pT40_6b-1-mix_5-4b_SPANet_pairing.h5'\n",
    "    label_path = sig_dir / f'gghhh-4pT40_6b-1.h5'\n",
    "    rename_dataset(file_path)\n",
    "    replace_labels(file_path, label_path)\n",
    "\n",
    "file_path = './Sample/SPANet/TRSM/pp6b-4pT40_4b-test-mix_5-4b_SPANet_pairing.h5'\n",
    "label_path = './Sample/SPANet/bkg/pp6b-4pT40_4b-test.h5'\n",
    "rename_dataset(file_path)\n",
    "replace_labels(file_path, label_path)\n",
    "\n",
    "file_path = './Sample/SPANet/TRSM/pp6b-4pT40_6b-1-mix_5-4b_SPANet_pairing.h5'\n",
    "label_path = './Sample/SPANet/bkg/pp6b-4pT40_6b-1.h5'\n",
    "rename_dataset(file_path)\n",
    "replace_labels(file_path, label_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
