{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import shutil\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_keys(f):\n",
    "    # 取得所有 Dataset 的名稱\n",
    "    keys = []\n",
    "    f.visit(lambda key: keys.append(key) if isinstance(f[key], h5py.Dataset) else None)\n",
    "    return keys\n",
    "\n",
    "\n",
    "def print_h5_info(file, event=0):\n",
    "    # 印出所有 Dataset\n",
    "    print(file)\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        dataset_keys = get_dataset_keys(f)\n",
    "        print('Dataset size:', f[dataset_keys[0]].shape[0])\n",
    "        for key in dataset_keys:\n",
    "            print(key, end=' ')\n",
    "            print(f[key][event])\n",
    "\n",
    "\n",
    "def split_h5_size(main_file, size=1000):\n",
    "    # 將輸入的 HDF5 檔案以 size 分成兩個\n",
    "    root, ext = os.path.splitext(main_file)\n",
    "    split_file1 = root + '_split1' + ext\n",
    "    split_file2 = root + '_split2' + ext\n",
    "\n",
    "    with h5py.File(main_file, 'r') as f_main:\n",
    "        dataset_keys = get_dataset_keys(f_main)\n",
    "        key0 = dataset_keys[0]\n",
    "        total_size = f_main[key0].shape[0]\n",
    "        print(f'Size of {main_file}: {total_size}')\n",
    "        \n",
    "        if size > total_size:\n",
    "            print(f'Split size {size} is greater than the input file size {total_size}.')\n",
    "            \n",
    "        with h5py.File(split_file1, 'w') as f_sp1, h5py.File(split_file2, 'w') as f_sp2:    \n",
    "            sp_size = size\n",
    "            for key in dataset_keys:\n",
    "                maxShape = list(f_main[key].maxshape)\n",
    "                maxShape[0] = None\n",
    "                f_sp1.create_dataset(key, maxshape=maxShape, data=f_main[key][:sp_size])\n",
    "                f_sp2.create_dataset(key, maxshape=maxShape, data=f_main[key][sp_size:])\n",
    "            \n",
    "            print(f'Size of {split_file1}: {f_sp1[key0].shape[0]}')\n",
    "            print(f'Size of {split_file2}: {f_sp2[key0].shape[0]}')\n",
    "\n",
    "            \n",
    "def split_h5_file(main_file, r=0.9):\n",
    "    # 將輸入的 HDF5 檔案以 r 的比例分成兩個\n",
    "    root, ext = os.path.splitext(main_file)\n",
    "    split_file1 = root + '_split1' + ext\n",
    "    split_file2 = root + '_split2' + ext\n",
    "\n",
    "    with h5py.File(main_file, 'r') as f_main:\n",
    "        dataset_keys = get_dataset_keys(f_main)\n",
    "        key0 = dataset_keys[0]\n",
    "        total_size = f_main[key0].shape[0]\n",
    "        print(f'Size of {main_file}: {total_size}')\n",
    "        \n",
    "        size = int(total_size * r)\n",
    "        if size > total_size:\n",
    "            print(f'Split size {size} is greater than the input file size {total_size}.')\n",
    "\n",
    "        with h5py.File(split_file1, 'w') as f_sp1, h5py.File(split_file2, 'w') as f_sp2:    \n",
    "            sp_size = size\n",
    "            for key in dataset_keys:\n",
    "                maxShape = list(f_main[key].maxshape)\n",
    "                maxShape[0] = None\n",
    "                f_sp1.create_dataset(key, maxshape=maxShape, data=f_main[key][:sp_size])\n",
    "                f_sp2.create_dataset(key, maxshape=maxShape, data=f_main[key][sp_size:])\n",
    "                    \n",
    "            print(f'Size of {split_file1}: {f_sp1[key0].shape[0]}')\n",
    "            print(f'Size of {split_file2}: {f_sp2[key0].shape[0]}')\n",
    "    \n",
    "\n",
    "def merge_h5_file(main_file, *arg):\n",
    "    # 合併傳入的 HDF5 檔案    \n",
    "    \n",
    "    # 檢查傳入檔案結構是否都相同\n",
    "    same_structure = True\n",
    "    with h5py.File(main_file, 'r') as f_main:\n",
    "        main_dataset_keys = get_dataset_keys(f_main)\n",
    "        for append_file in arg:\n",
    "            with h5py.File(append_file, 'r') as f_append:\n",
    "                append_dataset_keys = get_dataset_keys(f_append)    \n",
    "                if set(main_dataset_keys) != set(append_dataset_keys):\n",
    "                    same_structure = False\n",
    "                    print(f\"'{main_file}' and '{append_file}' are not same structure, can not be merged.\")\n",
    "                    break\n",
    "\n",
    "    # 檢查檔案結構是否都相同\n",
    "    if not same_structure:\n",
    "        return\n",
    "    print(f\"'{main_file}' and {arg} are same structure, can be merged.\")\n",
    "\n",
    "    root, ext = os.path.splitext(main_file)\n",
    "    new_file = root + '_merged' + ext\n",
    "\n",
    "    # 檢查合併檔案是否存在\n",
    "    if os.path.isfile(new_file):\n",
    "        print(f'{new_file} exist. Can not copy {main_file} to {new_file}')\n",
    "        return\n",
    "\n",
    "    print(f'{new_file} not exist. Copy {main_file} to {new_file}')\n",
    "    shutil.copyfile(main_file, new_file)\n",
    "\n",
    "    with h5py.File(new_file, 'a') as f_main:\n",
    "        key0 = main_dataset_keys[0]\n",
    "        total_size = f_main[key0].shape[0]\n",
    "        print(f'Size of {main_file}: {total_size}')\n",
    "\n",
    "        for append_file in arg:\n",
    "            with h5py.File(append_file, 'r') as f_append:\n",
    "                size_of_append = f_append[key0].shape[0]            \n",
    "                print(f'Size of {append_file}: {size_of_append}')\n",
    "\n",
    "                total_size += size_of_append   \n",
    "                for key in main_dataset_keys:\n",
    "                    f_main[key].resize(total_size, axis=0)\n",
    "                    f_main[key][-size_of_append:] = f_append[key]\n",
    "                \n",
    "                print(f'Size of {new_file}: {f_main[key0].shape[0]}')\n",
    "    return new_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_particle_mask(quark_jet, particle_quarks):\n",
    "    # quark_jet: 每個夸克對應的 jet 編號，shape 為 (n_event, 6)\n",
    "    # particle_quarks: 粒子對應的夸克編號，shape 為 (n_quarks,)\n",
    "\n",
    "    # 檢查是否每個夸克都有對應的 jet\n",
    "    mask1 = np.all(quark_jet[:, particle_quarks] != -1, axis=1)\n",
    "\n",
    "    # 對每一個事件，檢查每個夸克對應的 jet 都不重複\n",
    "    count = np.array([[np.sum(event == event[i]) for i in particle_quarks] for event in quark_jet])\n",
    "    mask2 = np.all(count == 1, axis=1)\n",
    "\n",
    "    return mask1 & mask2\n",
    "\n",
    "\n",
    "def print_triHiggs_h5_info(file_path):\n",
    "    # 印出 triHiggs HDF5 資料中，各 Higgs 數目的事件數\n",
    "    print(file_path)\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "\n",
    "        h1b1 = f['TARGETS/h1/b1'][...]\n",
    "        h1b2 = f['TARGETS/h1/b2'][...]\n",
    "        h2b1 = f['TARGETS/h2/b1'][...]\n",
    "        h2b2 = f['TARGETS/h2/b2'][...]\n",
    "        h3b1 = f['TARGETS/h3/b1'][...]\n",
    "        h3b2 = f['TARGETS/h3/b2'][...]\n",
    "\n",
    "        quark_jet = np.array([h1b1, h1b2, h2b1, h2b2, h3b1, h3b2]).T\n",
    "\n",
    "        h1_mask = get_particle_mask(quark_jet, (0, 1))\n",
    "        h2_mask = get_particle_mask(quark_jet, (2, 3))\n",
    "        h3_mask = get_particle_mask(quark_jet, (4, 5))\n",
    "        \n",
    "        n_tot = h1_mask.shape[0]\n",
    "        n_0h = ((~h1_mask) & (~h2_mask) & (~h3_mask)).sum()\n",
    "        # 任一個 Higgs 有對應的 jet\n",
    "        n_1h = ((h1_mask & (~h2_mask) & (~h3_mask)) | \n",
    "                ((~h1_mask) & h2_mask & (~h3_mask)) | \n",
    "                ((~h1_mask) & (~h2_mask) & h3_mask)).sum()\n",
    "        \n",
    "        # 任兩個 Higgs 有對應的 jet\n",
    "        n_2h = ((h1_mask & h2_mask & (~h3_mask)) | \n",
    "                ((~h1_mask) & h2_mask & h3_mask) | \n",
    "                (h1_mask & (~h2_mask) & h3_mask)).sum()\n",
    "        n_3h = (h1_mask & h2_mask & h3_mask).sum()\n",
    "\n",
    "    print(f'Dataset size: {n_tot}')\n",
    "    print(f'Number of 0 Higgs events: {n_0h}')\n",
    "    print(f'Number of 1 Higgs events: {n_1h}')\n",
    "    print(f'Number of 2 Higgs events: {n_2h}')\n",
    "    print(f'Number of 3 Higgs events: {n_3h}')\n",
    "    \n",
    "    print(f'\\\\item Total sample size: {n_tot:,}')\n",
    "    print(f'\\\\item 1h sample size: {n_1h:,}')\n",
    "    print(f'\\\\item 2h sample size: {n_2h:,}')\n",
    "    print(f'\\\\item 3h sample size: {n_3h:,}')\n",
    "    \n",
    "    result = {\n",
    "        'total': n_tot,\n",
    "        '0h': n_0h,\n",
    "        '1h': n_1h,\n",
    "        '2h': n_2h,\n",
    "        '3h': n_3h\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得資料夾內所有檔案的檔名，返回路徑\n",
    "def get_all_files(folder, ext=None):\n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(folder):\n",
    "        for filename in filenames:\n",
    "            if ext:\n",
    "                if filename.endswith(ext):\n",
    "                    files.append(os.path.join(root, filename))\n",
    "            else:\n",
    "                files.append(os.path.join(root, filename))\n",
    "    return files\n",
    "\n",
    "\n",
    "files = get_all_files('Sample/h5')\n",
    "merged_h5 = merge_h5_file(files[0], *files[1:])\n",
    "new_file = 'Sample/h5/background.h5'\n",
    "\n",
    "os.rename(merged_h5, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample/SPANet/gghhh_0b_01.h5\n",
      "Dataset size: 52061\n",
      "CLASSIFICATIONS/EVENT/signal 1\n",
      "INPUTS/Source/MASK [ True  True  True  True  True  True False False False False False False\n",
      " False False False]\n",
      "INPUTS/Source/btag [False  True  True  True False False False False False False False False\n",
      " False False False]\n",
      "INPUTS/Source/eta [ 0.01805974 -0.9333278  -0.08137434  1.2160542  -1.7853693  -0.31661886\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "INPUTS/Source/mass [23.57334   18.395107   6.7061844 14.5643     6.147192   4.2319746\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.       ]\n",
      "INPUTS/Source/phi [ 0.7539842   1.285823    2.8136048   0.17197756 -0.63893867 -1.8587049\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "INPUTS/Source/pt [92.083786 78.457825 51.525585 46.00039  33.061516 27.063663  0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.      ]\n",
      "TARGETS/h1/b1 1\n",
      "TARGETS/h1/b2 2\n",
      "TARGETS/h2/b1 5\n",
      "TARGETS/h2/b2 0\n",
      "TARGETS/h3/b1 -1\n",
      "TARGETS/h3/b2 -1\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Sample/SPANet/gghhh_0b_01.h5'\n",
    "print_h5_info(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample/SPANet/gghhh_0b_02.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 522899\n",
      "Number of 0 Higgs events: 82190\n",
      "Number of 1 Higgs events: 184769\n",
      "Number of 2 Higgs events: 161476\n",
      "Number of 3 Higgs events: 94464\n",
      "\\item Total sample size: 522,899\n",
      "\\item 1h sample size: 184,769\n",
      "\\item 2h sample size: 161,476\n",
      "\\item 3h sample size: 94,464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': 522899, '0h': 82190, '1h': 184769, '2h': 161476, '3h': 94464}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'Sample/SPANet/gghhh_0b_02.h5'\n",
    "print_triHiggs_h5_info(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Sample/SPANet/gghhh_0b_02.h5: 522899\n",
      "Size of Sample/SPANet/gghhh_0b_02_split1.h5: 470609\n",
      "Size of Sample/SPANet/gghhh_0b_02_split2.h5: 52290\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Sample/SPANet/gghhh_0b_02.h5'\n",
    "r = 0.9\n",
    "split_h5_file(file_path, r)\n",
    "\n",
    "root, ext = os.path.splitext(file_path)\n",
    "split_file1 = root + '_split1' + ext\n",
    "split_file2 = root + '_split2' + ext\n",
    "\n",
    "os.rename(split_file1, '../SPANet2/data/triHiggs/gghhh_0b_train.h5')\n",
    "os.rename(split_file2, '../SPANet2/data/triHiggs/gghhh_0b_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../SPANet2/data/triHiggs/gghhh_0b_train.h5\n",
      "Dataset size: 470609\n",
      "CLASSIFICATIONS/EVENT/signal 1\n",
      "INPUTS/Source/MASK [ True  True  True  True  True  True False False False False False False\n",
      " False False False]\n",
      "INPUTS/Source/btag [False False  True False False False False False False False False False\n",
      " False False False]\n",
      "INPUTS/Source/eta [-0.6438533   1.0463842   0.7722511   2.124152   -0.17052057  1.7725807\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "INPUTS/Source/mass [16.58891  12.245288  7.338748 13.670185  8.0877    8.103769  0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.      ]\n",
      "INPUTS/Source/phi [ 2.3027658  -0.5379352   0.29748732  2.5127425  -2.0321293   1.7991484\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "INPUTS/Source/pt [118.89806  100.983536  79.25531   47.51311   37.596863  36.342655\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.      ]\n",
      "TARGETS/h1/b1 3\n",
      "TARGETS/h1/b2 1\n",
      "TARGETS/h2/b1 -1\n",
      "TARGETS/h2/b2 -1\n",
      "TARGETS/h3/b1 4\n",
      "TARGETS/h3/b2 5\n",
      "../SPANet2/data/triHiggs/gghhh_0b_test.h5\n",
      "Dataset size: 52290\n",
      "CLASSIFICATIONS/EVENT/signal 1\n",
      "INPUTS/Source/MASK [ True  True  True  True  True  True False False False False False False\n",
      " False False False]\n",
      "INPUTS/Source/btag [ True  True  True  True  True False False False False False False False\n",
      " False False False]\n",
      "INPUTS/Source/eta [1.8520167  1.5393035  1.4989125  1.1356697  1.199997   0.27388152\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "INPUTS/Source/mass [ 9.419477  10.436351   3.9688392  3.0473928  7.239222   5.1911564\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.       ]\n",
      "INPUTS/Source/phi [-0.3980905   2.1905677   2.9549868  -2.2204542  -1.0482541  -0.41033673\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "INPUTS/Source/pt [68.80725  65.637825 43.615307 41.831535 35.95934  29.244717  0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.      ]\n",
      "TARGETS/h1/b1 3\n",
      "TARGETS/h1/b2 0\n",
      "TARGETS/h2/b1 5\n",
      "TARGETS/h2/b2 2\n",
      "TARGETS/h3/b1 4\n",
      "TARGETS/h3/b2 1\n"
     ]
    }
   ],
   "source": [
    "print_h5_info('../SPANet2/data/triHiggs/gghhh_0b_train.h5')\n",
    "print_h5_info('../SPANet2/data/triHiggs/gghhh_0b_test.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
